#!/usr/bin/env python3
# app.py

import json
import os
import re
import time
import uuid
from pathlib import Path
from typing import Any, Dict, List, Tuple

import pandas as pd
import requests
import streamlit as st

# =========================================
# Settings helpers
# =========================================
SETTINGS_PATH = Path("settings.json")


def _defaults() -> dict:
    return {
        "gateway_url": "",
        "access_token": "",
        "sql_api": {
            "token": "",
            "auth_mode": "Bearer (OAuth/PAT)",  # or "Snowflake Token"
            "warehouse": "WORKSHOP_WH",
            "database": "MOVIES",
            "schema": "PUBLIC",
            "role": "CORTEX_AGENT_ROLE",
        },
    }


def load_settings() -> dict:
    if SETTINGS_PATH.exists():
        try:
            data = json.loads(SETTINGS_PATH.read_text())
            base = _defaults()
            base.update({k: data.get(k, base[k]) for k in base})
            if isinstance(data.get("sql_api"), dict):
                base["sql_api"].update(data["sql_api"])
            return base
        except Exception:
            return _defaults()
    return _defaults()


def save_settings(settings: dict) -> None:
    SETTINGS_PATH.write_text(json.dumps(settings, indent=2))


# =========================================
# SSE parsing
# =========================================
def normalize_payload(s: str) -> str:
    """Repeatedly unescape until 'data:' lines are readable."""
    if not isinstance(s, str):
        return ""
    r = s
    for _ in range(5):
        if "data:" in r and "\n" in r:
            break
        try:
            r = json.loads(f'"{r}"')  # JSON unescape
            continue
        except Exception:
            pass
        try:
            r = bytes(r, "utf-8").decode("unicode_escape")  # unicode escape
        except Exception:
            break
    r = r.replace("\\r\\n", "\n").replace("\\n", "\n")
    return r

def sanitize_sql(sql: str) -> str:
    """
    Keep it to one clean statement for SQL API.
    - Remove the Analyst comment line
    - Strip trailing semicolons and whitespace
    """
    s = sql or ""
    # remove the "Generated by Cortex Analyst" line or any comment-only tail
    s = re.sub(r"(?mi)^\s*--\s*Generated by Cortex Analyst\s*$", "", s)
    # remove any other full-line comments if you want (optional):
    # s = re.sub(r"(?m)^\s*--.*$", "", s)
    # strip trailing semicolons and whitespace
    s = re.sub(r";\s*$", "", s.strip(), flags=re.S)
    return s


def parse_stream_to_struct(resp_json: dict) -> dict:
    """
    Parse the escaped SSE returned inside result.content[0].text.
    Supports envelopes:
      {"response":{"payload":{"content":"event: ..."}}}
      {"content":"event: ..."}
    Returns:
      {"answer": str, "sql": [str], "tables": [{"columns":[...],"rows":[...]}],
       "search_snippets":[str], "search_rows":[{text,doc_title,doc_id,source_id}], "raw": resp_json}
    """
    out = {
        "answer": "",
        "sql": [],
        "tables": [],
        "search_snippets": [],
        "search_rows": [],
        "raw": resp_json,
    }

    items = (resp_json or {}).get("result", {}).get("content", []) or []
    text_blob = next((c.get("text", "") for c in items if c.get("type") == "text"), "")
    if not isinstance(text_blob, str) or not text_blob.strip():
        out["answer"] = "(no answer text)"
        return out

    # unwrap the inner JSON that holds the SSE content string
    payload = ""
    try:
        env = json.loads(text_blob)
        payload = (
            env.get("response", {}).get("payload", {}) or {}
        ).get("content") or env.get("content") or ""
    except Exception:
        payload = text_blob
    if not isinstance(payload, str) or not payload:
        out["answer"] = "(no answer text)"
        return out

    # normalize/unescape until 'data:' lines are readable
    payload = normalize_payload(payload)

    answer_chunks: List[str] = []
    sql_list: List[str] = []
    tables: List[Dict[str, Any]] = []
    search_snips: List[str] = []
    search_rows: List[Dict[str, Any]] = []
    final_answer_text: str = ""

    current_event = ""
    for line in payload.splitlines():
        line = line.strip()
        if not line:
            continue
        if line.startswith("event:"):
            current_event = line.split(":", 1)[1].strip()
            continue
        if not line.startswith("data:"):
            continue
        data_str = line[5:].strip()
        if data_str in ("[DONE]", "DONE", ""):
            continue

        ev = None
        for _ in range(2):
            try:
                ev = json.loads(data_str)
                break
            except Exception:
                try:
                    data_str = (
                        data_str.encode("utf-8").decode("unicode_escape").replace('\\"', '"')
                    )
                except Exception:
                    break
        if not isinstance(ev, dict):
            continue

        # Prefer the final assembled response
        if current_event == "response" and ev.get("role") and isinstance(ev.get("content"), list):
            for item in ev.get("content"):
                if item.get("type") == "text" and isinstance(item.get("text"), str):
                    final_answer_text = item["text"].strip() or final_answer_text
            continue

        # Streamed assistant text
        if current_event == "response.text.delta" and isinstance(ev.get("text"), str):
            t = ev.get("text") or ""
            if t.strip():
                answer_chunks.append(t.rstrip())
            continue

        # Ignore thinking deltas explicitly
        if current_event == "response.thinking.delta":
            continue

        # Tool results with JSON (tables, SQL)
        if current_event == "response.tool_result" and isinstance(ev.get("content"), list):
            for c in (ev.get("content") or []):
                ctype = c.get("type")
                if ctype == "json":
                    j = c.get("json") or {}
                    if isinstance(j.get("sql"), str) and j["sql"].strip():
                        sql_list.append(j["sql"].strip())
                    # Support both sql_exec_result and result_set shapes
                    rs = j.get("sql_exec_result") or j.get("result_set")
                    if isinstance(rs, dict):
                        ser = rs
                        row_type = ((ser.get("resultSetMetaData") or {}).get("rowType")) or []
                        cols = [c.get("name") for c in row_type if isinstance(c, dict)]
                        rows = ser.get("data") or []
                        if rows:
                            tables.append({"columns": cols or [], "rows": rows})
                    if isinstance(j.get("text"), str) and j["text"].strip():
                        search_snips.append(j["text"].strip())
            continue

        # Legacy message.delta fallback
        for itm in (ev.get("delta") or {}).get("content") or []:
            typ = itm.get("type")

            # assistant text
            if typ in ("text", "output_text"):
                t = itm.get("text", "")
                if isinstance(t, str) and t.strip():
                    # Preserve newlines so Markdown lists/headings render nicely
                    answer_chunks.append(t.rstrip())

            # tool_use: capture SQL from sql_exec input.query (IMPORTANT)
            elif typ == "tool_use":
                tu = itm.get("tool_use") or {}
                if (tu.get("type") or "").lower() == "sql_exec":
                    raw_q = (((tu.get("input") or {}).get("query")) or "")
                    if isinstance(raw_q, str) and raw_q.strip():
                        try:
                            # unescape the query (it often arrives with \\n etc)
                            raw_q = bytes(raw_q, "utf-8").decode("unicode_escape")
                        except Exception:
                            pass
                        sql_list.append(raw_q.strip())

            # tool_results: look for json.sql and streamed tables/search
            elif typ == "tool_results":
                for c in (itm.get("tool_results") or {}).get("content") or []:
                    ctype = c.get("type")

                    if ctype == "text":
                        t = c.get("text", "")
                        if isinstance(t, str) and t.strip():
                            search_snips.append(t.strip())

                    elif ctype == "json":
                        j = c.get("json") or {}
                        # Analyst SQL is often here too
                        if isinstance(j.get("sql"), str) and j["sql"].strip():
                            sql_list.append(j["sql"].strip())

                        cols = j.get("columns") or j.get("headers")
                        rows = j.get("rows") or j.get("records")
                        if rows:
                            tables.append({"columns": cols or [], "rows": rows})

                        if isinstance(j.get("searchResults"), list):
                            for r in j["searchResults"]:
                                if isinstance(r, dict):
                                    row = {
                                        "text": r.get("text")
                                        or r.get("snippet")
                                        or r.get("chunk_text")
                                        or r.get("matched_text"),
                                        "doc_title": r.get("doc_title"),
                                        "doc_id": r.get("doc_id"),
                                        "source_id": r.get("source_id"),
                                    }
                                    if row["text"]:
                                        search_rows.append(row)
                                        search_snips.append(row["text"])

                        if isinstance(j.get("text"), str) and j["text"].strip():
                            search_snips.append(j["text"].strip())

                    elif ctype == "table":
                        tbl = c.get("table") or {}
                        cols = tbl.get("headers") or tbl.get("columns") or []
                        rows = tbl.get("rows") or []
                        if rows:
                            tables.append({"columns": cols, "rows": rows})

    # finalize answer with fallbacks
    # Prefer the final full answer if present; otherwise join deltas
    answer = (final_answer_text.strip() if final_answer_text else "\n".join(answer_chunks).strip())
    if not answer and search_snips:
        answer = " ".join(search_snips[:2]).strip()
    if not answer and tables:
        cols = tables[0].get("columns") or []
        row = (tables[0].get("rows") or [None])[0]
        if row is not None:
            if isinstance(row, dict):
                cols = list(row.keys())
                vals = [row[k] for k in cols]
            else:
                vals = list(row)
            answer = "Top result: " + ", ".join(
                f"{cols[i] if i < len(cols) else f'col{i+1}'}={v}" for i, v in enumerate(vals)
            )
    if not answer:
        hits = re.findall(r'"text"\s*:\s*"([^"]+)"', payload, flags=re.DOTALL)
        cleaned = []
        for t in hits:
            t1 = (
                t.replace('\\"', '"')
                .replace("\\n", " ")
                .replace("\\r", " ")
                .replace("\\t", " ")
            )
            t1 = re.sub(r"\s+", " ", t1).strip()
            if len(t1.split()) > 4:
                cleaned.append(t1)
        if cleaned:
            answer = " ".join(cleaned[:2])

    out.update(
        {
            "answer": (answer or "(no answer text)"),
            "sql": sql_list,
            "tables": tables,
            "search_snippets": search_snips,
            "search_rows": search_rows,
        }
    )
    return out


# =========================================
# Answer formatting helpers
# =========================================
def format_answer_markdown(answer: str) -> str:
    """Heuristically improve readability for Markdown rendering.

    - Preserve bullets that come in as "•" by converting to Markdown "- "
    - Promote common section headers to bold labels
    - Trim excessive spaces and ensure blank line before sections
    """
    if not isinstance(answer, str) or not answer:
        return answer

    s = answer
    # Normalize Windows newlines and stray spacing
    s = s.replace("\r\n", "\n")
    # Convert bullet characters into markdown bullets
    s = re.sub(r"\s*•\s+", "\n- ", s)
    # Bold known headings
    for hdr in [
        "Top-rated genres",
        "Top rated genres",
        "Mid-range genres",
        "Lower-rated genres",
        "Lower rated genres",
        "Answer",
        "Analysis",
    ]:
        s = re.sub(fr"\s*{re.escape(hdr)}\s*:\s*", f"\n\n**{hdr}:**\n", s)
    # If a line has many ", " separated label:value pairs, break to bullets
    lines = []
    for line in s.split("\n"):
        if ":" in line and "," in line and len(line) > 120:
            parts = [p.strip() for p in line.split(",") if p.strip()]
            if sum(1 for p in parts if ":" in p) >= 3:
                lines.append("\n".join(f"- {p}" for p in parts))
                continue
        lines.append(line)
    s = "\n".join(lines)
    return s.strip()

# =========================================
# Snowflake SQL API client
# =========================================
def _build_api_headers(auth_mode: str, token: str) -> Dict[str, str]:
    if auth_mode == "Snowflake Token":
        return {
            "Authorization": f'Snowflake Token="{token}"',
            "Accept": "application/json",
            "Content-Type": "application/json",
        }
    return {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json",
        "Content-Type": "application/json",
    }


def execute_sql_via_sql_api(
    account_url: str,
    sql: str,
    token: str,
    auth_mode: str,
    role: str,
    warehouse: str,
    database: str,
    schema: str,
    timeout_s: int = 60,
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    base = account_url.split("://")[-1].rstrip("/")
    endpoint = f"https://{base}/api/v2/statements"
    req_id = str(uuid.uuid4())

    params = {
        "requestId": req_id,
        "roleName": role or "",
        "warehouse": warehouse or "",
        "database": database or "",
        "schema": schema or "",
        "timeout": str(timeout_s),
    }
    params = {k: v for k, v in params.items() if v}

    body = {"statement": sql, "resultSetMetaData": {"format": "json"}}
    headers = _build_api_headers(auth_mode, token)

    r = requests.post(endpoint, params=params, headers=headers, json=body)
    if r.status_code == 401 and auth_mode == "Bearer (OAuth/PAT)":
        headers = _build_api_headers("Snowflake Token", token)
        r = requests.post(endpoint, params=params, headers=headers, json=body)
    if r.status_code not in (200, 202):
        raise RuntimeError(f"SQL API submit failed: {r.status_code} {r.text}")

    j = r.json()
    if "data" in j and "resultSetMetaData" in j:
        cols = [c["name"] for c in j["resultSetMetaData"]["rowType"]]
        rows = j.get("data") or []
        return pd.DataFrame(rows, columns=cols), j

    handle = j.get("statementHandle")
    if not handle:
        return pd.DataFrame(), j

    poll_url = f"{endpoint}/{handle}"
    t0 = time.time()
    while True:
        pr = requests.get(poll_url, headers=headers, params={"resultSetMetaData": "FULL"})
        if pr.status_code == 401 and auth_mode == "Bearer (OAuth/PAT)":
            headers = _build_api_headers("Snowflake Token", token)
            pr = requests.get(poll_url, headers=headers, params={"resultSetMetaData": "FULL"})

        if pr.status_code != 200:
            raise RuntimeError(f"SQL API poll failed: {pr.status_code} {pr.text}")

        pj = pr.json()
        status = (pj.get("status") or "").lower()
        if status in ("success", "succeeded", "complete"):
            cols = [c["name"] for c in pj["resultSetMetaData"]["rowType"]]
            rows = pj.get("data") or []
            return pd.DataFrame(rows, columns=cols), pj
        if status.startswith("failed"):
            raise RuntimeError(f"SQL API error: {pj}")

        if time.time() - t0 > timeout_s:
            raise TimeoutError("SQL API timed out while polling.")
        time.sleep(0.5)


# =========================================
# Bedrock Gateway call
# =========================================
def call_cortex_agent(
    gateway_url: str,
    access_token: str,
    tool_name: str,
    account_url: str,
    query: str,
    model: str,
    database: str,
    schema: str,
    agent_name: str,
):
    formatted_account_url = account_url.split("://")[-1].rstrip("/")

    system_prompt = (
        "You are a helpful data analyst. "
        "For quantitative questions, use the semantic view; "
        "for unstructured content, use Cortex Search. Always produce a concise final answer."
    )

    # Preferred: new Agents REST API (runAgent)
    new_arguments = {
        "account_url": formatted_account_url,
        "database": database,
        "schema": schema,
        "agent": agent_name,
        "model": model,
        "Accept": "application/json",
        "messages": [
            {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            {"role": "user", "content": [{"type": "text", "text": query}]},
        ],
    }

    headers = {"Content-Type": "application/json", "Accept": "application/json", "Authorization": f"Bearer {access_token}"}
    payload_new = {"jsonrpc": "2.0", "id": "streamlit", "method": "tools/call", "params": {"name": tool_name, "arguments": new_arguments}}
    resp = requests.post(gateway_url, headers=headers, json=payload_new)

    # Fallback to legacy /cortex/agent:run if the gateway doesn't have the new op
    if resp.status_code == 404 or (resp.status_code == 400 and "runAgent" in (tool_name or "")):
        legacy_tool = tool_name.replace("runAgent", "runCortexAgent") if tool_name else "SnowflakeCortexTarget___runCortexAgent"
        legacy_arguments = {
            "account_url": formatted_account_url,
            "model": model,
            "messages": [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
                {"role": "user", "content": [{"type": "text", "text": query}]},
            ],
            "tool_choice": "auto",
            "tools": [
                {"tool_spec": {"type": "cortex_analyst_text_to_sql", "name": "data_model"}},
                {"tool_spec": {"type": "sql_exec", "name": "sql_exec"}},
                {"tool_spec": {"type": "cortex_search", "name": "cortex_search"}},
            ],
            "tool_resources": {
                "data_model": {"semantic_view": "MOVIES.PUBLIC.MOVIES_SEMANTIC_VIEW"},
                "cortex_search": {"name": "MOVIES.PUBLIC.MOVIE_SEARCH", "max_results": 5},
                "sql_exec": {
                    "role": "CORTEX_AGENT_ROLE",
                    "warehouse": "WORKSHOP_WH",
                    "database": "MOVIES",
                    "schema": "PUBLIC",
                },
            },
        }
        payload_legacy = {"jsonrpc": "2.0", "id": "streamlit", "method": "tools/call", "params": {"name": legacy_tool, "arguments": legacy_arguments}}
        resp = requests.post(gateway_url, headers=headers, json=payload_legacy)

    return resp


# =========================================
# Streamlit UI
# =========================================
st.set_page_config(page_title="Cortex Agents Demo", page_icon="🎬", layout="wide")
st.title("🎬 Cortex Agents Demo")

settings = load_settings()

# Sidebar settings
st.sidebar.header("Gateway settings")
gateway_url = st.sidebar.text_input("Gateway URL", value=settings.get("gateway_url", ""), placeholder="https://<id>.<region>.amazonaws.com/mcp")
access_token = st.sidebar.text_input("Access Token", value=settings.get("access_token", ""), type="password")
tool_name = st.sidebar.text_input("Tool Name", value="SnowflakeCortexTarget___runAgent")
model = st.sidebar.text_input("Model", value="claude-4-sonnet")

st.sidebar.markdown("---")
st.sidebar.subheader("Snowflake SQL API")
# resolve token automatically (env → saved → sidebar)
env_token = os.getenv("SNOWFLAKE_SQL_API_TOKEN") or os.getenv("SNOWFLAKE_OAUTH_TOKEN")
saved_token = (settings.get("sql_api") or {}).get("token") or ""
resolved_token = env_token or saved_token
remember_token = st.sidebar.checkbox("Remember token in settings.json (unencrypted)", value=True)
sf_api_token = st.sidebar.text_input("SQL API token", value=resolved_token, type="password")
auth_mode = st.sidebar.selectbox(
    "Auth header",
    ["Bearer (OAuth/PAT)", "Snowflake Token"],
    index=0 if (settings.get("sql_api", {}).get("auth_mode", "Bearer (OAuth/PAT)") == "Bearer (OAuth/PAT)") else 1,
)
sf_warehouse = st.sidebar.text_input("Warehouse", value=settings.get("sql_api", {}).get("warehouse", "WORKSHOP_WH"))
sf_database = st.sidebar.text_input("Database", value=settings.get("sql_api", {}).get("database", "MOVIES"))
sf_schema = st.sidebar.text_input("Schema", value=settings.get("sql_api", {}).get("schema", "PUBLIC"))
sf_role = st.sidebar.text_input("Role", value=settings.get("sql_api", {}).get("role", "CORTEX_AGENT_ROLE"))

if st.sidebar.button("Save settings"):
    new_settings = {
        "gateway_url": gateway_url,
        "access_token": access_token,
        "sql_api": {
            "token": sf_api_token if remember_token else "",
            "auth_mode": auth_mode,
            "warehouse": sf_warehouse,
            "database": sf_database,
            "schema": sf_schema,
            "role": sf_role,
        },
    }
    save_settings(new_settings)
    st.sidebar.success("Saved to settings.json")

# Main inputs
account_url = st.text_input("Snowflake account URL (e.g., myacct.snowflakecomputing.com)")
db_input = st.text_input("Database (for Agent)", value=sf_database)
schema_input = st.text_input("Schema (for Agent)", value=sf_schema)
agent_input = st.text_input("Agent name", value="TESTAGENTUI")
question = st.text_area("Your question", height=100, placeholder="e.g., Average rating by title? Monthly revenue trend?")

# Action
if st.button("Ask"):
    if not (gateway_url and access_token and tool_name and account_url and db_input and schema_input and agent_input and question):
        st.error("Please fill in all fields (Gateway URL, Access Token, Tool Name, Account URL, Database, Schema, Agent, and question).")
    else:
        with st.spinner("Calling Cortex Agent..."):
            resp = call_cortex_agent(
                gateway_url,
                access_token,
                tool_name,
                account_url,
                question,
                model=model,
                database=db_input,
                schema=schema_input,
                agent_name=agent_input,
            )

        if resp.status_code != 200:
            st.error(f"Request failed: {resp.status_code}")
            st.code(resp.text, language="json")
        else:
            data = resp.json()
            parsed = parse_stream_to_struct(data)

            st.subheader("Answer")
            md = format_answer_markdown(parsed["answer"]) if parsed["answer"] else "(no answer)"
            # Render as Markdown to preserve bullets, headings, and formatting
            st.markdown(md)

            # Analyst SQL: show and auto-run if token available
            if parsed["sql"]:
                last_sql = sanitize_sql(parsed["sql"][-1])
                st.subheader("Generated SQL")
                st.code(last_sql, language="sql")

                auto_token = (
                    sf_api_token
                    or os.getenv("SNOWFLAKE_SQL_API_TOKEN")
                    or os.getenv("SNOWFLAKE_OAUTH_TOKEN")
                    or (settings.get("sql_api", {}).get("token") or "")
                )
                if auto_token:
                    try:
                        with st.spinner("Running SQL via Snowflake SQL API..."):
                            df, _ = execute_sql_via_sql_api(
                                account_url=account_url,
                                sql=last_sql,
                                token=auto_token,
                                auth_mode=auth_mode,
                                role=sf_role,
                                warehouse=sf_warehouse,
                                database=sf_database,
                                schema=sf_schema,
                            )
                        st.subheader("Results (via SQL API)")
                        if df.empty:
                            st.info("No rows.")
                        else:
                            st.dataframe(df, use_container_width=True)
                    except Exception as e:
                        st.error(f"SQL API execution failed: {e}")
                else:
                    st.warning(
                        "No SQL API token found. Set SNOWFLAKE_SQL_API_TOKEN or enter/save a token in the sidebar."
                    )

            # Any tabular rows streamed by the agent
            if parsed["tables"]:
                st.subheader("Results (from agent stream)")
                first = parsed["tables"][0]
                cols = first.get("columns") or []
                rows = first.get("rows") or []
                if rows and isinstance(rows[0], dict):
                    df_agent = pd.DataFrame(rows)
                else:
                    df_agent = pd.DataFrame(rows, columns=cols if cols else None)
                st.dataframe(df_agent, use_container_width=True)

            # Clean Search results (rows with text/title/id/source)
            if parsed.get("search_rows"):
                st.subheader("Search results")
                df_search = pd.DataFrame(parsed["search_rows"])
                st.dataframe(df_search, use_container_width=True)
            elif parsed.get("search_snippets"):
                st.subheader("Search snippets")
                for s in parsed["search_snippets"][:10]:
                    st.markdown(f"- {s}")

            with st.expander("Raw streaming payload"):
                st.code(json.dumps(data, indent=2), language="json")
